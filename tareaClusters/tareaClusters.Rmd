---
title: "Clusters"
author: 
  - "Briseyda Amancaya"
  - "María Anciones Polo"
  - "Laura Gil García"
  - "José Miguel Hernández Cabrera"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

paquetes = c("cluster", "factoextra", "ggplot2")

por_instal = paquetes[!paquetes %in% installed.packages()[, 1]]

if(length(por_instal) > 0) install.packages(por_instal)
```

## Introducción
En este documento realizaremos un análisis de conglomerados o cluster.
## Paquetes

Los paquetes utilizados para realizar este análisis de clúster son `stats` y `cluster`. Además, utilizaremos las funciones `stats::hclust()`, `stats:dist()` y `cluster::diana()` para el análisis y los paquetes `factoextra` y `ggplot2` para la visualización de los grupos formados.

## Descripción de datos

En primer lugar, importamos los datos teniendo en cuenta que hay que adaptar el directorio donde se encuentran los datos, en cuyo caso se encuentran en la carpeta `data/`.

```{r}
paises = foreign::read.spss("data/PaisesProteinas.sav", to.data.frame = TRUE)
```

Posteriormente, podemos hacer una descriptiva básica de las distintas variables que componen la base de datos mediante la función `summary()`. 

```{r}
summary(paises)
```

Seleccionamos la variable de `paises` para empezar el análisis. Como los métodos cluster son muy sensibles al hecho de que las variables no estén todas medidas en las mismas unidades, es necesario escalar las variables numéricas para que todas las variables tengan la misma importancia en el análisis. 

La escala hace que todas las variables tengan media `0` y varianza `1`. Esto se realiza para evitar que el algoritmo de agrupamiento dependa de una unidad variable arbitraria.

```{r}
# Selección de variables numéricas
var.paises = paises[, 2:ncol(paises)]

# Crear matriz de estandarización
p.esc = scale(var.paises)

# Nombrar las filas con los nombres de los países
rownames(p.esc) = paises[, 1]

# Ver las primeras 6 observaciones
head(p.esc)
```

### Detección de atípicos

La siguiente fase consiste en detectar si existen observaciones aberrantes o atípicas que puedan influir en el modelo.

```{r}
boxplot(p.esc)
```

El boxplot detecta dos países con alto consumo de carne roja y uno de pescado, así como dos de bajo consumo de huevo. Para saber cuáles, utilizamos el método Tukey para detectar los atípicos, el cual consiste en:

$$[Q_1 - k(Q_3 - Q_1), Q_3 + k(Q_3 - Q_1)]$$

Para detectar de cuáles países se tratan creamos la función `detec_atip()`:

```{r}
detec_atip = function(x) {
  resultado = list()
  
  # Rangos de los Q1 y Q3
  ran.int = function(x) quantile(x, c(0.25, 0.75))
  
  # Detección usando el método de Tukey
  inferior = function(x) ran.int(x)[1] - (1.5 * IQR(x))
  superior = function(x) ran.int(x)[2] + (1.5 * IQR(x))
  
  # Escribir resultados en la lista creada
  resultado$ext.inferior = subset(x, x < inferior(x))
  resultado$ext.superior = subset(x, x > superior(x))
  
  return(resultado)
}

unlist(apply(p.esc, 2, detec_atip))
```

La función nos dice que `Francia` y el `Reino Unido` tienen un consumo alto atípico de carne roja. En el mismo sentido, `Portugal` consuma más pescado de lo normal. Por otra parte, `Portugal` y `Albania` consumen mucho menos huevo que el resto de los 23 países considerados.

Generalmente, se recomienda hacer el análisis considerando a los atípicos y sin ellos o realizar un tratamiento de atípicos. No obstante, también es necesario notar que los datos en realidad pueden corresponder a una realidad que va más allá de lo que pueda explicar el modelo.

Para efectos del ejercicio, dejamos esas observaciones sin modificaciones.

### Colinealidad

Para tener una correcta interpretación de los grupos formados, debemos asegurarnos que no haya colinealidad. En caso contrario, se puede optar por eliminarla o utilizar distancias que amortigüen la colinealidad.

```{r}
corrplot::corrplot(cor(p.esc))
```

Podemos observar que `Cereales` y `FrutosSecos` están inversamente correlacionados con todas las demás variables. A su vez, `Huevos`  y `Leche` tienen una relación lineal positiva con con `CarneRoja` y `CarneBlanca`.

Dado que el modelo de clusters requiere que no exista colinealidad, probablemente sea necesaria una reducción de dimensiones o eliminar las variables con alta correlación. No obstante, para efectos del ejercicio, procederemos con los datos completos para mostrar cómo se comportan los objetos.

### Distancias

Hay diferentes tipos de distancias con sus propiedades particulares pero las más habituales son las siguientes: 

Distancia euclidia: es la medida de similaridad más utilizada frecuentemente. Se trata de la distancia más corta entre dos puntos. 

$$d_{euc}(x, y) = \sqrt{\displaystyle \sum_{i = 1}^n(x_i - y_i)^2}$$

Distancia de Manhattan:
$$d_{man}(x, y) = \displaystyle \sum_{i = 1}^n |(x_i - y_i)|$$

Utilizamos la distancia euclidia

```{r}
d.euc = dist(p.esc, method = "euclidean")
```

## Clasificación jerárquica

### Descripción

El análisis de cluster jerárquico se utiliza tanto para variables cuantitativas como para variables cualitativas. También se emplea si no se conoce el número de cluster o cuando el número de objetos no es muy grande. 

Puede subdividirse en **aglomerativos** (fases sucesivas de fusiones de los $n$ individuos) y en **divisivos** (particionan los $n$ individuos).

### Métodos de aglomeración

Hay diferentes métodos jerárquicos aglomerativos para el análisis de cluster:

Ward: no calcula distancias entre cluster pero forma un cluster que maximiza la homogeneidad intra cluster.

Método del vecino más próximo: la distancia entre grupos se caracteriza por la del par de individuos que está más cercano (un individuo de cada grupo).

Método del vecino más lejano: la distancia entre grupos es la mayor distancia entre pares de individuos (uno de cada grupo).

Grupo promedio o UPGMA: la distancia se calcula como la media entre todos los pares de individuos de cada grupo. 


```{r}
metodos = c("ward.D2",  # Ward
            "single",   # Vecinos más próximos
            "complete", # Vecinos más lejanos
            "average")  # Grupo promedio o UPGMA
names(metodos) = metodos

met.agl = lapply(metodos, function(x) hclust(d.euc, method = x))
```

Utilizamos el **coeficiente de aglomeración**, para saber cuál método se ajusta mejor:

```{r}
coe_agl = sapply(met.agl, cluster::coef.hclust)
coe_agl = round(coe_agl, digits = 2)
coe_agl
```

Podemos observar que de todos los métodos, el de Ward se ajusta más adecuadamente.

### Visualización con dendrogramas

Los resultados del método jerárquico se representa gráficamente mediante un dendrograma, donde se indican las fusiones o divisiones producidas en las fases sucesivas del análisis.  

```{r}
library(ggplot2)
library(factoextra)

ttl.met = c(
  "Método de Ward",
  "Vecino más próximo",
  "Vecino más Lejano",
  "Grupo promedio"
)

coe.met = c(paste("Coef. aglo.:", coe_agl), "", "")

dendros = function(x, ttl, stl) {
  graf = fviz_dend(
    x,
    horiz = T,
    main = ttl,
    sub = stl,
    xlab = "",
    ylab = "Altura",
    cex = 0.6
  )
  return(graf)
}


lapply(1:4, function(x) {
  dendros(met.agl[[x]], ttl = ttl.met[x], stl = coe.met[x])
})
```

### Método por disimilitud

Disimilitud

```{r}
library(cluster)

met.dis = diana(d.euc)

# Coeficiente de disimilitud
met.dis$dc
```

### Número óptimo de clusters

```{r}
g1 = fviz_nbclust(p.esc, FUN = hcut, method = "wss", k.max = 10) +
  ggtitle("A. Método codo")
g2 = fviz_nbclust(p.esc, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("B. Método silueta")
g3 = fviz_nbclust(p.esc, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("C. Método gap")
gridExtra::grid.arrange(g1, g2, g3, nrow = 1)
```

Coeficientes para método codo

```{r}
g1$data
```

### Visualización de grupos

Usamos 3 grupos, con distancia euclídea y algoritmo de Ward.

```{r}
fviz_dend(
  met.agl$ward.D2,
  horiz = TRUE,
  k = 3,
  rect = TRUE,
  rect_fill = TRUE,
  k_colors = "lancet",
  cex = 0.6,
  ylab = "Altura"
) +
  theme(title = element_blank())
```

### Variables que más influyen

```{r}
grupos = cutree(met.agl$ward.D2, k = 3)

aggregate(p.esc, by = list(Cluster = grupos), mean)
```

ANOVA de los grupos por cada variable

```{r}
paises_clust = data.frame(p.esc, grupos)
ANOVA = aov(grupos ~ ., data = paises_clust)

summary(ANOVA)
```


## K-medias

### Descripción

Conocemos *a priori* el número de clusters

### Obtención de k-medias

```{r}
k.medias = kmeans(p.esc, centers = 3, nstart = 25)

k.medias$centers

k.medias$size
```

### Exploración de clusters

```{r}
# Promedio de k-medias con respecto a los datos
pmd.p.km = aggregate(p.esc, by = list(Cluster = k.medias$cluster), mean)

pmd.p.km
```

Análisis de la varianza de clusters respecto a las variables.
```{r}
paises.km = data.frame(p.esc, Cluster = k.medias$cluster)

sapply(colnames(paises.km)[1:9], function(x) {
  summary(
    aov(formula(paste0("Cluster~",x)), data = paises.km)
  )
})
```

### Visualización de K-medias



```{r}
fviz_cluster(k.medias,
             data = p.esc,
             palette = "lancet",
             ellipse.type = "euclid",
             star.plot = T,
             repel = T,
             ggtheme = theme_light()) +
  theme(legend.position = "none",
        plot.title = element_blank())
```

## Método PAM

Partición Alrededor de Medioides

Usamos `cluster::pam()`

### Descripción

En lugar de usar k-means, usa mediodes.

```{r}
met.pam = pam(p.esc, k = 3)

met.pam$medoids
```


### Visualización de mediodes

```{r}

fviz_cluster(met.pam,
             palette = "lancet",
             ellipse.type = "t",
             repel = T,
             ggtheme = theme_light()) +
  theme(legend.position = "none",
        plot.title = element_blank())
```


