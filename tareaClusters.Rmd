---
title: "Clusters"
author: 
  - "Briseyda Amancaya"
  - "María Anciones Polo"
  - "Laura Gil García"
  - "José Miguel Hernández Cabrera"
output: pdf_document
lang: es
bibliography: ref/refm3.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

paquetes = c("cluster", "factoextra", "ggplot2")

por_instal = paquetes[!paquetes %in% installed.packages()[, 1]]

if(length(por_instal) > 0) install.packages(por_instal)
```

## Introducción

En este documento realizaremos un análisis de conglomerados o cluster.

## Paquetes

Los paquetes utilizados para realizar este análisis de clúster son `stats` y `cluster`. Además, utilizaremos las funciones `stats::hclust()`, `stats:dist()` y `cluster::diana()` para el análisis y los paquetes `factoextra` y `ggplot2` para la visualización de los grupos formados.

## Descripción de datos

En primer lugar, importamos los datos teniendo en cuenta que hay que adaptar el directorio donde se encuentran los datos, en cuyo caso se encuentran en la carpeta `data/`.

```{r}
paises = foreign::read.spss("data/PaisesProteinas.sav", to.data.frame = TRUE)
```

Posteriormente, podemos hacer una descriptiva básica de las distintas variables que componen la base de datos mediante la función `summary()`. 

```{r}
summary(paises)
```

Seleccionamos la variable de `paises` para empezar el análisis. Como los métodos cluster son muy sensibles al hecho de que las variables no estén todas medidas en las mismas unidades, es necesario escalar las variables numéricas para que todas las variables tengan la misma importancia en el análisis. 

La escala hace que todas las variables tengan media `0` y varianza `1`. Esto se realiza para evitar que el algoritmo de agrupamiento dependa de una unidad variable arbitraria.

```{r}
# Selección de variables numéricas
var.paises = paises[, 2:ncol(paises)]

# Crear matriz de estandarización
p.esc = scale(var.paises)

# Nombrar las filas con los nombres de los países
rownames(p.esc) = paises[, 1]

# Ver las primeras 6 observaciones
head(p.esc)
```

### Detección de atípicos

La siguiente fase consiste en detectar si existen observaciones aberrantes o atípicas que puedan influir en el modelo.

```{r}
boxplot(p.esc)
```

El boxplot detecta dos países con alto consumo de carne roja y uno de pescado, así como dos de bajo consumo de huevo. Para saber cuáles, utilizamos el método Tukey para detectar los atípicos, el cual consiste en:

$$[Q_1 - k(Q_3 - Q_1), Q_3 + k(Q_3 - Q_1)]$$

Para detectar de cuáles países se tratan creamos la función `detec_atip()`:

```{r}
detec_atip = function(x) {
  resultado = list()
  
  # Rangos de los Q1 y Q3
  ran.int = function(x) quantile(x, c(0.25, 0.75))
  
  # Detección usando el método de Tukey
  inferior = function(x) ran.int(x)[1] - (1.5 * IQR(x))
  superior = function(x) ran.int(x)[2] + (1.5 * IQR(x))
  
  # Escribir resultados en la lista creada
  resultado$ext.inferior = subset(x, x < inferior(x))
  resultado$ext.superior = subset(x, x > superior(x))
  
  return(resultado)
}

unlist(apply(p.esc, 2, detec_atip))
```

La función nos dice que `Francia` y el `Reino Unido` tienen un consumo alto atípico de carne roja. En el mismo sentido, `Portugal` consuma más pescado de lo normal. Por otra parte, `Portugal` y `Albania` consumen mucho menos huevo que el resto de los 23 países considerados.

Generalmente, se recomienda hacer el análisis considerando a los atípicos y sin ellos o realizar un tratamiento de atípicos. No obstante, también es necesario notar que los datos en realidad pueden corresponder a una realidad que va más allá de lo que pueda explicar el modelo.

Para efectos del ejercicio, dejamos esas observaciones sin modificaciones.

### Colinealidad

Para tener una correcta interpretación de los grupos formados, debemos asegurarnos que no haya colinealidad. En caso contrario, se puede optar por eliminarla o utilizar distancias que amortigüen la colinealidad.

```{r}
corrplot::corrplot(cor(p.esc))
```

Podemos observar que `Cereales` y `FrutosSecos` están inversamente correlacionados con todas las demás variables. A su vez, `Huevos`  y `Leche` tienen una relación lineal positiva con con `CarneRoja` y `CarneBlanca`.

Dado que el modelo de clusters requiere que no exista colinealidad, probablemente sea necesaria una reducción de dimensiones o eliminar las variables con alta correlación. No obstante, para efectos del ejercicio, procederemos con los datos completos para mostrar cómo se comportan los objetos.

### Distancias

Hay diferentes tipos de distancias con sus propiedades particulares pero las más habituales son las siguientes: 

Distancia euclidia: es la medida de similaridad más utilizada frecuentemente. Se trata de la distancia más corta entre dos puntos. 

$$d_{euc}(x, y) = \sqrt{\displaystyle \sum_{i = 1}^n(x_i - y_i)^2}$$

Distancia de Manhattan:
$$d_{man}(x, y) = \displaystyle \sum_{i = 1}^n |(x_i - y_i)|$$

Utilizamos la distancia euclidia

```{r}
d.euc = dist(p.esc, method = "euclidean")
```

## Clasificación jerárquica

### Descripción

El análisis de cluster jerárquico se utiliza tanto para variables cuantitativas como para variables cualitativas. También se emplea si no se conoce el número de cluster o cuando el número de objetos no es muy grande. 

Puede subdividirse en **aglomerativos** (fases sucesivas de fusiones de los $n$ individuos) y en **divisivos** (particionan los $n$ individuos).

### Métodos de aglomeración

Hay diferentes métodos jerárquicos aglomerativos para el análisis de cluster:

Ward: no calcula distancias entre cluster pero forma un cluster que maximiza la homogeneidad intra cluster.

Método del vecino más próximo: la distancia entre grupos se caracteriza por la del par de individuos que está más cercano (un individuo de cada grupo).

Método del vecino más lejano: la distancia entre grupos es la mayor distancia entre pares de individuos (uno de cada grupo).

Grupo promedio o UPGMA: la distancia se calcula como la media entre todos los pares de individuos de cada grupo. 


```{r}
metodos = c("ward.D2",  # Ward
            "single",   # Vecinos más próximos
            "complete", # Vecinos más lejanos
            "average")  # Grupo promedio o UPGMA
names(metodos) = metodos

met.agl = lapply(metodos, function(x) hclust(d.euc, method = x))
```

Utilizamos el **coeficiente de aglomeración**, para saber cuál método se ajusta mejor:

```{r}
coe_agl = sapply(met.agl, cluster::coef.hclust)
coe_agl = round(coe_agl, digits = 2)
coe_agl
```

Podemos observar que de todos los métodos, el de Ward se ajusta más adecuadamente.

### Visualización con dendrogramas

Los resultados del método jerárquico se representa gráficamente mediante un dendrograma, donde se indican las fusiones o divisiones producidas en las fases sucesivas del análisis.  

```{r}
library(ggplot2)
library(factoextra)

ttl.met = c(
  "Método de Ward",
  "Vecino más próximo",
  "Vecino más Lejano",
  "Grupo promedio"
)

coe.met = c(paste("Coef. aglo.:", coe_agl), "", "")

dendros = function(x, ttl, stl) {
  graf = fviz_dend(
    x,
    horiz = T,
    main = ttl,
    sub = stl,
    xlab = "",
    ylab = "Altura",
    cex = 0.6
  )
  return(graf)
}


lapply(1:4, function(x) {
  dendros(met.agl[[x]], ttl = ttl.met[x], stl = coe.met[x])
})
```

### Método por disimilitud

La mayoría de las funciones de clasificaciones jerárquicas se enfocan en el método de aglomeración ofreciendo diversos métodos, por lo que el método de disimilitud es autocontenido.

El algoritmo construye los grupos a partir de uno solo que englobe a todas las $n$ observaciones. Luego, se realizan diversas etapas en las que los conglomerados son divididos hasta que contenga una sola observación, seleccionando aquellos que tengan el diámetro más grande (la disimilitud). 

Para dividir el cluster seleccionado en cada etapa, el algoritmo busca la observación con la disimilud promedio más grande del grupo. Esta observación inicia el modo "grupo de fragmentación", en donde el algoritmo reasignará las observaciones que están más cercanas a este grupo que al conglomerado inicial. De tal forma que al final de ese paso se han formado dos nuevos clusters.

Para calcularlo en R, usamos la función `cluster::diana()` para iniciar la clasificación jerárquica por disimilitud. Cabe señalar que además del vector, la función también brinda el coeficiente de disimilitud, análogo al de aglomeración.

```{r}
library(cluster)

met.dis = diana(d.euc)

# Coeficiente de disimilitud
met.dis$dc
```

Podemos ver que el coeficiente de disimilitud es similar al método del vecino más lejano.

### Número óptimo de clusters

Existen técnicas para decidir cuántos grupos es recomendable utilizar. Tres de ellas son el método de sedimentos (o de codo), de silueta [@ROUSSEEUW1987] y del estadístico *gap* [@Tibshirani_etal2001]. Para obtenerlos, utilizamos la función `factoextra::fviz_nbclust()`.

```{r}
g1 = fviz_nbclust(p.esc, FUN = hcut, method = "wss", k.max = 10) +
  labs(title = "A. Sedimentos", x = "No. de clusters", y = "Suma de cuadrados intra grupos")
g2 = fviz_nbclust(p.esc, FUN = hcut, method = "silhouette", k.max = 10) +
  labs(title = "A. Silueta", x = "No. de clusters", y = "Ancho promedio de la silueta")
g3 = fviz_nbclust(p.esc, FUN = hcut, method = "gap_stat", k.max = 10) +
  labs(title = "A. Gap", x = "No. de clusters", y = "Estadístico gap (k)")
gridExtra::grid.arrange(g1, g2, g3, nrow = 1)
```

Los tres métodos arrojan resultados distintos, pero tanto el método de sedimentos como el de silueta sugieren 3 clusters. Cabe señalar que SPSS utiliza el método silueta para decidir el número óptimo de grupos.

En adelante utilizaremos 3 grupos.

### Visualización de grupos

Usamos 3 grupos, con distancia euclídea y algoritmo de Ward.

```{r}
fviz_dend(
  met.agl$ward.D2,
  horiz = TRUE,
  k = 3,
  rect = TRUE,
  rect_fill = TRUE,
  k_colors = "lancet",
  cex = 0.6,
  ylab = "Altura"
)
```

Observamos que en un grupo se encuentran los países asociados a la dieta mediterránea (`España`, `Portugal`, `Italia` y `Grecia`), los países balcánicos (`Yugoslavia`, `Rumania`, `Bulgaria` y `Albania`) y el resto.

### Variables que más influyen

Vistos los grupos, veamos qué influye más en la conformación de cada uno.

```{r}
grupos = cutree(met.agl$ward.D2, k = 3)

sapply(1:3, function(x) trimws(names(grupos[grupos == x])))
```

En el cluster 1 están los balcánicos, el cluster 3 los mediterráneos y el resto en el cluster 2.

```{r}
influencia = aggregate(p.esc, by = list(Cluster = grupos), mean)

sort(influencia[1, 2:10], decreasing = T)
sort(influencia[2, 2:10], decreasing = T)
sort(influencia[3, 2:10], decreasing = T)
```
 
Vemos que los balcanes se caracterizan por consumir más cereales y frutos secos; los meditarráneos consumen más frutos y vegetales, frutos secos, pescado y cereales; el resto tiene una dieta rica en carnes, huevos, lácteos y féculas.
 
Veamos ahora el ANOVA de los grupos por cada variable

```{r}
paises_clust = data.frame(p.esc, grupos)
ANOVA = aov(grupos ~ ., data = paises_clust)

summary(ANOVA)
```

Aunque vemos que las variables `Huevos`, `Pescado`, `FrutosSecos` y `FrutosyVegetales` son las que más influyem en el modelo, es necesario aclarar que la interpretación no es confiable debido a que no se cumplen todos los supuestos para realizar correctamente el ANOVA ni el cluster.

## K-medias

### Descripción

*El Algoritmo de K-medias* pertenece a los cluster *NO JERARQUICOS*. Esto implica un conocimiento *a priori* del número de cluster, mediante el cual asignación de cada observación al cluster más cercano y una parada del método si no se produce reasignación o si la reasignación satisface la regla de parada.

El objetivo del algoritmo de K-medias es separar las observaciones en k-cluster, de manera que cada dato pertenezca únicamente a un grupo y se maximice la homogeneidad dentro de cada uno de ellos.

### Obtención de k-medias

La función kmeans recibe tres parametros: p.esc (nuestros datos ya estandarizados),centers (número de grupos a formar), nstart (número de casos, en este caso paises).

Con la posterior sentencia se obtiene el peso que tiene cada una de nuestras variables en cada uno de los 3 grupos.

Y para finalizar, vemos el tamaño que tiene cada uno de los 3 grupos: el grupo 1 lo conforman 4 paises, el grupo 2 se compone de 6 paises y el grupo 3 de 15 paises.

```{r}
k.medias = kmeans(p.esc, centers = 3, nstart = 25)

k.medias$centers

k.medias$size
```

### Exploración de clusters

```{r}
# Promedio de k-medias con respecto a los datos
pmd.p.km = aggregate(p.esc, by = list(Cluster = k.medias$cluster), mean)

pmd.p.km
```

Análisis de la varianza de clusters respecto a las variables.
```{r}
paises.km = data.frame(p.esc, Cluster = k.medias$cluster)

sapply(colnames(paises.km)[1:9], function(x) {
  summary(
    aov(formula(paste0("Cluster~",x)), data = paises.km)
  )
})
```

### Visualización de K-medias

```{r}
fviz_cluster(k.medias,
             data = p.esc,
             palette = "lancet",
             ellipse.type = "euclid",
             star.plot = T,
             repel = T,
             ggtheme = theme_light()) +
  theme(legend.position = "none",
        plot.title = element_blank())
```

## Método de PAM

El método Partición Alrededor de Medoides (PAM), propuesto por @park2009, al igual que K-Means, se encuentra en el grupo de métodos de agrupamiento por particiones, y en el cual se utiliza medianas en vez de medias con el objetivo de limitar la influencia de los atípicos.

Para encontrar k clusters, el modelo PAM determina un objeto representativo para cada clúster. Este objeto representativo, llamado "medoid", es el que se encuentra localizado más al centro dentro del clúster. Una vez que los medoids han sido seleccionados, cada objeto no seleccionado es agrupado con el medoid al cual es más similar. De este modo, la partición se realiza alrededor de medoides. 

### Descripción

Inicialmente determinamos el número de clusters a formar, en este caso 3 agrupaciones. En cada iteración, se realiza un intercambio entre un objeto seleccionado y un objeto no seleccionado si y solo si el intercambio resulta en un incremento de la calidad del agrupamiento (clustering).

Para calcularlo en R, usamos `cluster::pam()`

```{r}
met.pam = pam(p.esc, k = 3)

met.pam$medoids
```

Podemos observar que PAM de `k = 3` coloca a `Rumania`, `Bélgica` y `España` como los `medoids` (i.e., los centros) de sus respectivos grupos.

### Visualización de mediodes

Para obtener una representación gráfica del clustering, se ha empleado en este caso la función `factoextra::fviz_cluster()`.

```{r}
fviz_cluster(met.pam,
             palette = "lancet",
             ellipse.type = "t",
             repel = T,
             ggtheme = theme_light()) +
  theme(legend.position = "none",
        plot.title = element_blank())
```

Los clusters muestran claramente su formación alrededor de los medoides y no centralidades como en K-means. Aunque los resultados son similares, mediante el método PAM se visualiza con mayor especificidad los grupos conformados.

## Ventajas y desventajas del método PAM

El método PAM es un método de clustering más robusto que K-means, por tanto es más adecuado para datos que contengan atípicos o ruido.

No obstante, al igual que K-means, necesita que se especifique de antemano el número de clusters que se van a conformar. Esto puede ser complicado de determinar si no se dispone de los grupos *a priori* del análisis.

Finalmente, si bien su funcionamiento es adecuado con datos de $n$ pequeñas, no es escalable debido a su complejidad computacional.

***
